{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1.post2\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpus = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/\"\n",
    "with open(f'{path}tfidf_matrix.pkl', 'rb') as fh:\n",
    "    tfidf = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2785</th>\n",
       "      <th>2786</th>\n",
       "      <th>2787</th>\n",
       "      <th>2788</th>\n",
       "      <th>2789</th>\n",
       "      <th>2790</th>\n",
       "      <th>2791</th>\n",
       "      <th>2792</th>\n",
       "      <th>2793</th>\n",
       "      <th>2794</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2795 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   2785  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "\n",
       "   2786  2787  2788  2789  2790  2791  2792  2793  2794  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 2795 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = pd.DataFrame(tfidf.toarray()); tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26744, 2795)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_movies, n_tokens = tfidf.shape; n_movies, n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxVal = tfidf.values.max(); minVal = tfidf.values.min()\n",
    "maxVal, minVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AETrainingData(Dataset):\n",
    "    '''\n",
    "        Format the training dataset to be input into the auto encoder.\n",
    "        Takes in dataframe and converts it to a PyTorch Tensor\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, x_train):\n",
    "        self.x = x_train\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "            Returns a example from the data set as a pytorch tensor.\n",
    "        '''\n",
    "        # Get example/target pair at idx as numpy arrays\n",
    "        x, y = self.x.iloc[idx].values, self.x.iloc[idx].values\n",
    "\n",
    "        # Convert to torch tensor\n",
    "        x = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "        y = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "        \n",
    "        # Return pair        \n",
    "        return {'input': x, 'target': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_tokens, 1000),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1000, 100),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(100, 1000),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1000, n_tokens),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(input_tensor, target_tensor, encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    # clear the gradients in the optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass through \n",
    "    \n",
    "    encoded_representation = encoder(input_tensor)\n",
    "    reconstruction = decoder(encoded_representation)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(reconstruction, target_tensor)\n",
    "    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step the optimizers to update the model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    # Return the loss value to track training progress\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, dataloader, epochs, print_every_n_batches=100, learning_rate=0.0001):\n",
    "    \n",
    "    # keep track of losses\n",
    "    plot_losses = []\n",
    "\n",
    "    # Initialize Encoder Optimizer\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
    "    \n",
    "    # Initialize Decoder Optimizer\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
    "\n",
    "    # Specify loss function\n",
    "    criterion = nn.MSELoss(reduction='elementwise_mean')\n",
    "    \n",
    "    # Cycle through epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        # Cycle through batches\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            \n",
    "            input_tensor = batch['input'].cuda()\n",
    "            target_tensor = batch['target'].cuda()\n",
    "            \n",
    "\n",
    "            loss = train_ae(input_tensor, target_tensor, encoder, decoder,\n",
    "                         encoder_optimizer, decoder_optimizer, criterion)\n",
    "            \n",
    "\n",
    "            if i % print_every_n_batches == 0 and i != 0:\n",
    "                print(loss)\n",
    "                plot_losses.append(loss)\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AETrainingData(tfidf)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder().cuda()\n",
    "decoder = decoder().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "0.00025159408687613904\n",
      "0.0002567078627180308\n",
      "0.0002347471017856151\n",
      "0.0002623775217216462\n",
      "Epoch 2/30\n",
      "0.0002566072216723114\n",
      "0.00025404829648323357\n",
      "0.00025616673519834876\n",
      "0.00027566897915676236\n",
      "Epoch 3/30\n",
      "0.00022701302077621222\n",
      "0.00026205735048279166\n",
      "0.00026155426166951656\n",
      "0.0002506707387510687\n",
      "Epoch 4/30\n",
      "0.0002485751756466925\n",
      "0.000243923015659675\n",
      "0.00021555279090534896\n",
      "0.00024902125005610287\n",
      "Epoch 5/30\n",
      "0.000250117271207273\n",
      "0.00026644422905519605\n",
      "0.00021542387548834085\n",
      "0.00020635980763472617\n",
      "Epoch 6/30\n",
      "0.0002318520419066772\n",
      "0.00024445666349492967\n",
      "0.0002500811533536762\n",
      "0.0002262728230562061\n",
      "Epoch 7/30\n",
      "0.00022470254043582827\n",
      "0.0002388863795204088\n",
      "0.00023566283925902098\n",
      "0.0002182702737627551\n",
      "Epoch 8/30\n",
      "0.00025422341423109174\n",
      "0.0002464283024892211\n",
      "0.00022828884539194405\n",
      "0.00024847075110301375\n",
      "Epoch 9/30\n",
      "0.00020504763233475387\n",
      "0.00023537101515103132\n",
      "0.0001939756766660139\n",
      "0.00023538354435004294\n",
      "Epoch 10/30\n",
      "0.00023264429182745516\n",
      "0.00024739952641539276\n",
      "0.00021929822105448693\n",
      "0.0002488992176949978\n",
      "Epoch 11/30\n",
      "0.0002571389195509255\n",
      "0.000244109018240124\n",
      "0.0002186748170061037\n",
      "0.0002524727024137974\n",
      "Epoch 12/30\n",
      "0.0002843722468242049\n",
      "0.0002034297212958336\n",
      "0.00022960851492825896\n",
      "0.0002163077297154814\n",
      "Epoch 13/30\n",
      "0.0002466692531015724\n",
      "0.00022991953301243484\n",
      "0.00023643516760785133\n",
      "0.000230883524636738\n",
      "Epoch 14/30\n",
      "0.00022978676133789122\n",
      "0.00022776717378292233\n",
      "0.00021825054136570543\n",
      "0.00021961520542390645\n",
      "Epoch 15/30\n",
      "0.00021018930419813842\n",
      "0.00020868689171038568\n",
      "0.00022300089767668396\n",
      "0.00022534973686560988\n",
      "Epoch 16/30\n",
      "0.00020100998517591506\n",
      "0.0002334899181732908\n",
      "0.00021807575831189752\n",
      "0.0002181088348152116\n",
      "Epoch 17/30\n",
      "0.0002161924639949575\n",
      "0.00023466830316465348\n",
      "0.00022460715263150632\n",
      "0.00021264335373416543\n",
      "Epoch 18/30\n",
      "0.0002233714476460591\n",
      "0.00022935928427614272\n",
      "0.0002104860031977296\n",
      "0.00021942394960206002\n",
      "Epoch 19/30\n",
      "0.0002068163303192705\n",
      "0.00024355333880521357\n",
      "0.0001768100046319887\n",
      "0.00022947440447751433\n",
      "Epoch 20/30\n",
      "0.00021245652169454843\n",
      "0.00024377263616770506\n",
      "0.00021841234411112964\n",
      "0.00019227323355153203\n",
      "Epoch 21/30\n",
      "0.00019713469373527914\n",
      "0.00020336065790615976\n",
      "0.0002087044413201511\n",
      "0.00020883789693471044\n",
      "Epoch 22/30\n",
      "0.0001982615503948182\n",
      "0.0002004159614443779\n",
      "0.00025590634322725236\n",
      "0.00023937338846735656\n",
      "Epoch 23/30\n",
      "0.00023117722594179213\n",
      "0.00021131891116965562\n",
      "0.0001606693840585649\n",
      "0.00019460998009890318\n",
      "Epoch 24/30\n",
      "0.00020168200717307627\n",
      "0.00019477818568702787\n",
      "0.00021135898714419454\n",
      "0.00019297568360343575\n",
      "Epoch 25/30\n",
      "0.00020679998851846904\n",
      "0.00020203609892632812\n",
      "0.00020648620557039976\n",
      "0.00023574900114908814\n",
      "Epoch 26/30\n",
      "0.00022871441615279764\n",
      "0.00023603915178682655\n",
      "0.00021548425138462335\n",
      "0.0002275264705531299\n",
      "Epoch 27/30\n",
      "0.0002258671447634697\n",
      "0.00021850966732017696\n",
      "0.0002184145851060748\n",
      "0.00020505243446677923\n",
      "Epoch 28/30\n",
      "0.0002107048057951033\n",
      "0.00017186769400723279\n",
      "0.00021183623175602406\n",
      "0.00018564499623607844\n",
      "Epoch 29/30\n",
      "0.00023688589863013476\n",
      "0.00018406515300739557\n",
      "0.00021531808306463063\n",
      "0.00019671341578941792\n",
      "Epoch 30/30\n",
      "0.00022206221183296293\n",
      "0.00019637470541056246\n",
      "0.00020832759037148207\n",
      "0.0002301732893101871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00025159408687613904,\n",
       " 0.0002567078627180308,\n",
       " 0.0002347471017856151,\n",
       " 0.0002623775217216462,\n",
       " 0.0002566072216723114,\n",
       " 0.00025404829648323357,\n",
       " 0.00025616673519834876,\n",
       " 0.00027566897915676236,\n",
       " 0.00022701302077621222,\n",
       " 0.00026205735048279166,\n",
       " 0.00026155426166951656,\n",
       " 0.0002506707387510687,\n",
       " 0.0002485751756466925,\n",
       " 0.000243923015659675,\n",
       " 0.00021555279090534896,\n",
       " 0.00024902125005610287,\n",
       " 0.000250117271207273,\n",
       " 0.00026644422905519605,\n",
       " 0.00021542387548834085,\n",
       " 0.00020635980763472617,\n",
       " 0.0002318520419066772,\n",
       " 0.00024445666349492967,\n",
       " 0.0002500811533536762,\n",
       " 0.0002262728230562061,\n",
       " 0.00022470254043582827,\n",
       " 0.0002388863795204088,\n",
       " 0.00023566283925902098,\n",
       " 0.0002182702737627551,\n",
       " 0.00025422341423109174,\n",
       " 0.0002464283024892211,\n",
       " 0.00022828884539194405,\n",
       " 0.00024847075110301375,\n",
       " 0.00020504763233475387,\n",
       " 0.00023537101515103132,\n",
       " 0.0001939756766660139,\n",
       " 0.00023538354435004294,\n",
       " 0.00023264429182745516,\n",
       " 0.00024739952641539276,\n",
       " 0.00021929822105448693,\n",
       " 0.0002488992176949978,\n",
       " 0.0002571389195509255,\n",
       " 0.000244109018240124,\n",
       " 0.0002186748170061037,\n",
       " 0.0002524727024137974,\n",
       " 0.0002843722468242049,\n",
       " 0.0002034297212958336,\n",
       " 0.00022960851492825896,\n",
       " 0.0002163077297154814,\n",
       " 0.0002466692531015724,\n",
       " 0.00022991953301243484,\n",
       " 0.00023643516760785133,\n",
       " 0.000230883524636738,\n",
       " 0.00022978676133789122,\n",
       " 0.00022776717378292233,\n",
       " 0.00021825054136570543,\n",
       " 0.00021961520542390645,\n",
       " 0.00021018930419813842,\n",
       " 0.00020868689171038568,\n",
       " 0.00022300089767668396,\n",
       " 0.00022534973686560988,\n",
       " 0.00020100998517591506,\n",
       " 0.0002334899181732908,\n",
       " 0.00021807575831189752,\n",
       " 0.0002181088348152116,\n",
       " 0.0002161924639949575,\n",
       " 0.00023466830316465348,\n",
       " 0.00022460715263150632,\n",
       " 0.00021264335373416543,\n",
       " 0.0002233714476460591,\n",
       " 0.00022935928427614272,\n",
       " 0.0002104860031977296,\n",
       " 0.00021942394960206002,\n",
       " 0.0002068163303192705,\n",
       " 0.00024355333880521357,\n",
       " 0.0001768100046319887,\n",
       " 0.00022947440447751433,\n",
       " 0.00021245652169454843,\n",
       " 0.00024377263616770506,\n",
       " 0.00021841234411112964,\n",
       " 0.00019227323355153203,\n",
       " 0.00019713469373527914,\n",
       " 0.00020336065790615976,\n",
       " 0.0002087044413201511,\n",
       " 0.00020883789693471044,\n",
       " 0.0001982615503948182,\n",
       " 0.0002004159614443779,\n",
       " 0.00025590634322725236,\n",
       " 0.00023937338846735656,\n",
       " 0.00023117722594179213,\n",
       " 0.00021131891116965562,\n",
       " 0.0001606693840585649,\n",
       " 0.00019460998009890318,\n",
       " 0.00020168200717307627,\n",
       " 0.00019477818568702787,\n",
       " 0.00021135898714419454,\n",
       " 0.00019297568360343575,\n",
       " 0.00020679998851846904,\n",
       " 0.00020203609892632812,\n",
       " 0.00020648620557039976,\n",
       " 0.00023574900114908814,\n",
       " 0.00022871441615279764,\n",
       " 0.00023603915178682655,\n",
       " 0.00021548425138462335,\n",
       " 0.0002275264705531299,\n",
       " 0.0002258671447634697,\n",
       " 0.00021850966732017696,\n",
       " 0.0002184145851060748,\n",
       " 0.00020505243446677923,\n",
       " 0.0002107048057951033,\n",
       " 0.00017186769400723279,\n",
       " 0.00021183623175602406,\n",
       " 0.00018564499623607844,\n",
       " 0.00023688589863013476,\n",
       " 0.00018406515300739557,\n",
       " 0.00021531808306463063,\n",
       " 0.00019671341578941792,\n",
       " 0.00022206221183296293,\n",
       " 0.00019637470541056246,\n",
       " 0.00020832759037148207,\n",
       " 0.0002301732893101871]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainIters(encoder, decoder, dataloader, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.from_numpy(tfidf.values).type(torch.FloatTensor).cuda()\n",
    "test = test.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = encoder(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.cpu()\n",
    "pred = pred.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 26744, 100)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.reshape((pred.shape[1], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26744, 100)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/autoencoder_embeddings.pkl', 'wb') as fh:\n",
    "    pickle.dump(pred, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ana3",
   "language": "python",
   "name": "ana3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
